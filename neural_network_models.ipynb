{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from keras.layers import Conv1D, GRU, Dense, Dropout, Flatten, LSTM\n",
    "from keras.models import Sequential\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from keras.regularizers import l2\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "base_path = Path('dataset/dataset_versions')\n",
    "\n",
    "datasets = {}\n",
    "datasets_names = (\n",
    "    'bfill_ffill',\n",
    "    'linear_interpolation',\n",
    "    'cubic_interpolation',\n",
    "    'quadratic_interpolation',\n",
    "    'polynomial_5_interpolation',\n",
    "    'polynomial_7_interpolation',\n",
    "    'polynomial_9_interpolation',\n",
    "    'polynomial_11_interpolation',\n",
    ")\n",
    "for dataset_name in datasets_names:\n",
    "    dataset = pd.read_excel(base_path / f'{dataset_name}_rescaled_dataset.xlsx')\n",
    "    datasets[dataset_name] = dataset.iloc[:, 1:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "seed = 7\n",
    "target_feature_name = 'GDP per capita (current US$)'\n",
    "\n",
    "SplittedDataset = namedtuple('SplittedDataset', ['name', 'x_train', 'y_train', 'x_test', 'y_test'])\n",
    "splited_datasets = []\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    model = dict()\n",
    "    model['name'] = dataset_name\n",
    "    data_x = dataset.drop([target_feature_name], axis=1)\n",
    "    data_y = dataset[target_feature_name]\n",
    "    model['x_train'], model['x_test'], model['y_train'], model['y_test'] = train_test_split(data_x, data_y, test_size=test_size, random_state=seed)\n",
    "    splited_datasets.append(SplittedDataset(model['name'], model['x_train'],  model['y_train'], model['x_test'], model['y_test']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "Reloading Tuner from .\\models_for_polynomial_11_interpolation\\tuner0.json\n",
      "7/7 [==============================] - 8s 7ms/step - loss: 412.1651 - r2_score: -705.2758\n",
      "INFO:tensorflow:Assets written to: .\\models_for_polynomial_11_interpolation\\best_model\\assets\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to restore custom object of type _tf_keras_metric. Please make sure that any custom layers are included in the `custom_objects` arg when calling `load_model()` and make sure that all layers implement `get_config` and `from_config`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 19\u001B[0m\n\u001B[0;32m     17\u001B[0m clf \u001B[38;5;241m=\u001B[39m ak\u001B[38;5;241m.\u001B[39mStructuredDataRegressor(max_trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, project_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels_for_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39mr2_loss, metrics\u001B[38;5;241m=\u001B[39m[r2_score])\n\u001B[0;32m     18\u001B[0m clf\u001B[38;5;241m.\u001B[39mfit(dataset\u001B[38;5;241m.\u001B[39mx_train, dataset\u001B[38;5;241m.\u001B[39my_train)\n\u001B[1;32m---> 19\u001B[0m test_predict \u001B[38;5;241m=\u001B[39m \u001B[43mclf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m rmse \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msqrt(mean_squared_error(dataset\u001B[38;5;241m.\u001B[39my_test, test_predict))\n\u001B[0;32m     21\u001B[0m r2 \u001B[38;5;241m=\u001B[39m r2_score(dataset\u001B[38;5;241m.\u001B[39my_test, test_predict)\n",
      "File \u001B[1;32m~\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\autokeras\\tasks\\structured_data.py:165\u001B[0m, in \u001B[0;36mBaseStructuredDataPipeline.predict\u001B[1;34m(self, x, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Predict the output for a given testing data.\u001B[39;00m\n\u001B[0;32m    152\u001B[0m \n\u001B[0;32m    153\u001B[0m \u001B[38;5;124;03m# Arguments\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;124;03m    The predicted results.\u001B[39;00m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    163\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_for_predict(x)\n\u001B[1;32m--> 165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mpredict(x\u001B[38;5;241m=\u001B[39mx, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\autokeras\\auto_model.py:452\u001B[0m, in \u001B[0;36mAutoModel.predict\u001B[1;34m(self, x, batch_size, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    450\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_adapt(x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs, batch_size)\n\u001B[0;32m    451\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtuner\u001B[38;5;241m.\u001B[39mget_best_pipeline()\n\u001B[1;32m--> 452\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtuner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_best_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    453\u001B[0m dataset \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mtransform_x(dataset)\n\u001B[0;32m    454\u001B[0m dataset \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataset\u001B[38;5;241m.\u001B[39mzip((dataset, dataset))\n",
      "File \u001B[1;32m~\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\autokeras\\engine\\tuner.py:63\u001B[0m, in \u001B[0;36mAutoTuner.get_best_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_best_model\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m keras_tuner\u001B[38;5;241m.\u001B[39mengine\u001B[38;5;241m.\u001B[39mtuner\u001B[38;5;241m.\u001B[39mmaybe_distribute(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribution_strategy):\n\u001B[1;32m---> 63\u001B[0m         model \u001B[38;5;241m=\u001B[39m \u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbest_model_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[1;32m~\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\keras\\saving\\saved_model\\load.py:1137\u001B[0m, in \u001B[0;36mrevive_custom_object\u001B[1;34m(identifier, metadata)\u001B[0m\n\u001B[0;32m   1135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m revived_cls\u001B[38;5;241m.\u001B[39m_init_from_metadata(metadata)\n\u001B[0;32m   1136\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1137\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1138\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to restore custom object of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00midentifier\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1139\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease make sure that any custom layers are included in the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1140\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`custom_objects` arg when calling `load_model()` and make sure \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1141\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthat all layers implement `get_config` and `from_config`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1142\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Unable to restore custom object of type _tf_keras_metric. Please make sure that any custom layers are included in the `custom_objects` arg when calling `load_model()` and make sure that all layers implement `get_config` and `from_config`."
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import autokeras as ak\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    r2 = 1 - SS_res/(SS_tot + K.epsilon())\n",
    "    return r2\n",
    "\n",
    "# Кастомная функция потерь на основе r2\n",
    "def r2_loss(y_true, y_pred):\n",
    "    return -r2_score(y_true, y_pred)\n",
    "\n",
    "dataset = splited_datasets[-1]\n",
    "clf = ak.StructuredDataRegressor(max_trials=1, project_name=f'models_for_{dataset.name}', loss=r2_loss, metrics=[r2_score])\n",
    "clf.fit(dataset.x_train, dataset.y_train)\n",
    "test_predict = clf.predict(dataset.x_test)\n",
    "rmse = np.sqrt(mean_squared_error(dataset.y_test, test_predict))\n",
    "r2 = r2_score(dataset.y_test, test_predict)\n",
    "print(r2, rmse)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from .\\models_for_polynomial_11_interpolation\\tuner0.json\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Daniil_Alenushkin\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Daniil_Alenushkin\\AppData\\Local\\Temp\\ipykernel_46516\\1708412979.py\", line 8, in custom_r2_loss  *\n        r2 = r2_score(y_true.to_numpy(), y_pred.to_numpy())\n\n    AttributeError: 'Tensor' object has no attribute 'to_numpy'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m dataset \u001B[38;5;241m=\u001B[39m splited_datasets[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     13\u001B[0m clf \u001B[38;5;241m=\u001B[39m ak\u001B[38;5;241m.\u001B[39mStructuredDataRegressor(max_trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, project_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodels_for_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39mcustom_r2_loss)\n\u001B[1;32m---> 14\u001B[0m \u001B[43mclf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m test_predict \u001B[38;5;241m=\u001B[39m clf\u001B[38;5;241m.\u001B[39mpredict(dataset\u001B[38;5;241m.\u001B[39mx_test)\n\u001B[0;32m     16\u001B[0m rmse \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msqrt(mean_squared_error(dataset\u001B[38;5;241m.\u001B[39my_test, test_predict))\n",
      "File \u001B[1;32m~\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\autokeras\\tasks\\structured_data.py:139\u001B[0m, in \u001B[0;36mBaseStructuredDataPipeline.fit\u001B[1;34m(self, x, y, epochs, callbacks, validation_split, validation_data, **kwargs)\u001B[0m\n\u001B[0;32m    135\u001B[0m         validation_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_read_from_csv(x_val, y_val)\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_in_fit(x)\n\u001B[1;32m--> 139\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mfit(\n\u001B[0;32m    140\u001B[0m     x\u001B[38;5;241m=\u001B[39mx,\n\u001B[0;32m    141\u001B[0m     y\u001B[38;5;241m=\u001B[39my,\n\u001B[0;32m    142\u001B[0m     epochs\u001B[38;5;241m=\u001B[39mepochs,\n\u001B[0;32m    143\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    144\u001B[0m     validation_split\u001B[38;5;241m=\u001B[39mvalidation_split,\n\u001B[0;32m    145\u001B[0m     validation_data\u001B[38;5;241m=\u001B[39mvalidation_data,\n\u001B[0;32m    146\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    147\u001B[0m )\n\u001B[0;32m    148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m history\n",
      "File \u001B[1;32m~\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\autokeras\\auto_model.py:292\u001B[0m, in \u001B[0;36mAutoModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, callbacks, validation_split, validation_data, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validation_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m validation_split:\n\u001B[0;32m    288\u001B[0m     dataset, validation_data \u001B[38;5;241m=\u001B[39m data_utils\u001B[38;5;241m.\u001B[39msplit_dataset(\n\u001B[0;32m    289\u001B[0m         dataset, validation_split\n\u001B[0;32m    290\u001B[0m     )\n\u001B[1;32m--> 292\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtuner\u001B[38;5;241m.\u001B[39msearch(\n\u001B[0;32m    293\u001B[0m     x\u001B[38;5;241m=\u001B[39mdataset,\n\u001B[0;32m    294\u001B[0m     epochs\u001B[38;5;241m=\u001B[39mepochs,\n\u001B[0;32m    295\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    296\u001B[0m     validation_data\u001B[38;5;241m=\u001B[39mvalidation_data,\n\u001B[0;32m    297\u001B[0m     validation_split\u001B[38;5;241m=\u001B[39mvalidation_split,\n\u001B[0;32m    298\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    300\u001B[0m )\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m history\n",
      "File \u001B[1;32m~\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\autokeras\\engine\\tuner.py:220\u001B[0m, in \u001B[0;36mAutoTuner.search\u001B[1;34m(self, epochs, callbacks, validation_split, verbose, **fit_kwargs)\u001B[0m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhypermodel\u001B[38;5;241m.\u001B[39mset_fit_args(\u001B[38;5;241m0\u001B[39m, epochs\u001B[38;5;241m=\u001B[39mcopied_fit_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    219\u001B[0m     copied_fit_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mverbose\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m verbose\n\u001B[1;32m--> 220\u001B[0m     pipeline, model, history \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinal_fit(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcopied_fit_kwargs)\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    222\u001B[0m     \u001B[38;5;66;03m# TODO: Add return history functionality in Keras Tuner\u001B[39;00m\n\u001B[0;32m    223\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_best_models()[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\autokeras\\engine\\tuner.py:270\u001B[0m, in \u001B[0;36mAutoTuner.final_fit\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    268\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_best_model()\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madapt(model, kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m--> 270\u001B[0m model, history \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mfit_with_adaptive_batch_size(\n\u001B[0;32m    271\u001B[0m     model, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhypermodel\u001B[38;5;241m.\u001B[39mbatch_size, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    272\u001B[0m )\n\u001B[0;32m    273\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pipeline, model, history\n",
      "File \u001B[1;32m~\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\autokeras\\utils\\utils.py:88\u001B[0m, in \u001B[0;36mfit_with_adaptive_batch_size\u001B[1;34m(model, batch_size, **fit_kwargs)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_with_adaptive_batch_size\u001B[39m(model, batch_size, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs):\n\u001B[1;32m---> 88\u001B[0m     history \u001B[38;5;241m=\u001B[39m run_with_adaptive_batch_size(\n\u001B[0;32m     89\u001B[0m         batch_size, \u001B[38;5;28;01mlambda\u001B[39;00m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: model\u001B[38;5;241m.\u001B[39mfit(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs\n\u001B[0;32m     90\u001B[0m     )\n\u001B[0;32m     91\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model, history\n",
      "File \u001B[1;32m~\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\autokeras\\utils\\utils.py:101\u001B[0m, in \u001B[0;36mrun_with_adaptive_batch_size\u001B[1;34m(batch_size, func, **fit_kwargs)\u001B[0m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m batch_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    100\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 101\u001B[0m         history \u001B[38;5;241m=\u001B[39m func(x\u001B[38;5;241m=\u001B[39mx, validation_data\u001B[38;5;241m=\u001B[39mvalidation_data, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs)\n\u001B[0;32m    102\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    103\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m tf\u001B[38;5;241m.\u001B[39merrors\u001B[38;5;241m.\u001B[39mResourceExhaustedError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\autokeras\\utils\\utils.py:89\u001B[0m, in \u001B[0;36mfit_with_adaptive_batch_size.<locals>.<lambda>\u001B[1;34m(**kwargs)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_with_adaptive_batch_size\u001B[39m(model, batch_size, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs):\n\u001B[0;32m     88\u001B[0m     history \u001B[38;5;241m=\u001B[39m run_with_adaptive_batch_size(\n\u001B[1;32m---> 89\u001B[0m         batch_size, \u001B[38;5;28;01mlambda\u001B[39;00m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: model\u001B[38;5;241m.\u001B[39mfit(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_kwargs\n\u001B[0;32m     90\u001B[0m     )\n\u001B[0;32m     91\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model, history\n",
      "File \u001B[1;32m~\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32mC:\\Users\\DANIIL~1\\AppData\\Local\\Temp\\__autograph_generated_filetgn1irlx.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Users\\DANIIL~1\\AppData\\Local\\Temp\\__autograph_generated_filebd0h_dwc.py:10\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__custom_r2_loss\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m      8\u001B[0m do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m      9\u001B[0m retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mUndefinedReturnValue()\n\u001B[1;32m---> 10\u001B[0m r2 \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(r2_score), (ag__\u001B[38;5;241m.\u001B[39mconverted_call(\u001B[43mag__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mld\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_numpy\u001B[49m, (), \u001B[38;5;28;01mNone\u001B[39;00m, fscope), ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(y_pred)\u001B[38;5;241m.\u001B[39mto_numpy, (), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     12\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: in user code:\n\n    File \"C:\\Users\\Daniil_Alenushkin\\Desktop\\SUAI\\Magistracy_09.04.04\\predicting_a_country_economic_potential\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Daniil_Alenushkin\\AppData\\Local\\Temp\\ipykernel_46516\\1708412979.py\", line 8, in custom_r2_loss  *\n        r2 = r2_score(y_true.to_numpy(), y_pred.to_numpy())\n\n    AttributeError: 'Tensor' object has no attribute 'to_numpy'\n"
     ]
    }
   ],
   "source": [
    "import autokeras as ak\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "#for dataset in datasets:\n",
    "# TODO написать функцию r2 метрик - https://keras.io/api/metrics/\n",
    "#  и функцию r2 потерь - https://keras.io/api/losses/\n",
    "# Как то проверить, прогнать датасеты\n",
    "def custom_r2_loss(y_true: pd.DataFrame, y_pred: pd.DataFrame):\n",
    "    r2 = r2_score(y_true.to_numpy(), y_pred.to_numpy())\n",
    "    # Преобразование коэффициента детерминации в функцию потерь\n",
    "    return r2\n",
    "\n",
    "dataset = splited_datasets[-1]\n",
    "clf = ak.StructuredDataRegressor(max_trials=1, project_name=f'models_for_{dataset.name}', loss=custom_r2_loss)\n",
    "clf.fit(dataset.x_train, dataset.y_train)\n",
    "test_predict = clf.predict(dataset.x_test)\n",
    "rmse = np.sqrt(mean_squared_error(dataset.y_test, test_predict))\n",
    "r2 = r2_score(dataset.y_test, test_predict)\n",
    "print(r2, rmse)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 7s 8ms/step - loss: 0.0713 - mean_squared_error: 0.0713\n",
      "2/2 [==============================] - 4s 8ms/step\n",
      "-2.300531591882382 0.1093615226404732\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\n\\n# Получить информацию о модели\\nloaded_model.summary()\\n# Получить список слоев модели\\nlayers = loaded_model.layers\\n\\n# Пройтись по каждому слою и вывести его конфигурацию\\nfor layer in layers:\\n    print(layer.get_config())\\n    print()\\n'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import autokeras as ak\n",
    "# Загрузить модель из файла\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    r'structured_data_regressor\\best_model',\n",
    "     custom_objects={'MultiCategoryEncoding': ak.MultiCategoryEncoding}\n",
    ")\n",
    "dataset = splited_datasets[-1]\n",
    "loaded_model.fit(dataset.x_train, dataset.y_train)\n",
    "test_predict = loaded_model.predict(dataset.x_test)\n",
    "rmse = np.sqrt(mean_squared_error(dataset.y_test, test_predict))\n",
    "r2 = r2_score(dataset.y_test, test_predict)\n",
    "print(r2, rmse)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Получить информацию о модели\n",
    "loaded_model.summary()\n",
    "# Получить список слоев модели\n",
    "layers = loaded_model.layers\n",
    "\n",
    "# Пройтись по каждому слою и вывести его конфигурацию\n",
    "for layer in layers:\n",
    "    print(layer.get_config())\n",
    "    print()\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "param_grid_1d_gru = {\n",
    "    'filters': [8, 32],\n",
    "    'kernel_size': [1, 3],\n",
    "    'units': [16, 64],\n",
    "    'optimizer' : [\"adam\", \"rmsprop\"],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'gru_act': ['tanh', 'relu'],\n",
    "    'conv_act': ['tanh', 'relu']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 6s 340ms/step - loss: 1.8138 - r2_score: -2951.1140 - val_loss: 1.2088 - val_r2_score: -282.4652\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 2s 221ms/step - loss: 1.1275 - r2_score: -497.3425 - val_loss: 0.9268 - val_r2_score: -2.1547\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 2s 218ms/step - loss: 0.9813 - r2_score: -7.1266 - val_loss: 0.8985 - val_r2_score: -48.3351\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 2s 218ms/step - loss: 0.8915 - r2_score: -162.6429 - val_loss: 0.8047 - val_r2_score: -0.8301\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 2s 216ms/step - loss: 0.8325 - r2_score: -69.7763 - val_loss: 0.7617 - val_r2_score: -19.3501\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 2s 219ms/step - loss: 0.7697 - r2_score: -9.9717 - val_loss: 0.7103 - val_r2_score: -20.0519\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 2s 219ms/step - loss: 0.7169 - r2_score: -98.1435 - val_loss: 0.6479 - val_r2_score: -0.5087\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 2s 221ms/step - loss: 0.6619 - r2_score: -42.3922 - val_loss: 0.6158 - val_r2_score: -18.8198\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 2s 223ms/step - loss: 0.6257 - r2_score: -118.1682 - val_loss: 0.5695 - val_r2_score: -12.9207\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 2s 224ms/step - loss: 0.5788 - r2_score: -38.9684 - val_loss: 0.5260 - val_r2_score: -6.7482\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 2s 233ms/step - loss: 0.5439 - r2_score: -21.1771 - val_loss: 0.5024 - val_r2_score: -23.8785\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.5032 - r2_score: -49.1027 - val_loss: 0.4619 - val_r2_score: -14.5278\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 2s 262ms/step - loss: 0.4732 - r2_score: -49.5372 - val_loss: 0.4312 - val_r2_score: -14.9370\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 2s 260ms/step - loss: 0.4368 - r2_score: -14.7536 - val_loss: 0.4066 - val_r2_score: -20.5386\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.4159 - r2_score: -262.1648 - val_loss: 0.3782 - val_r2_score: -18.3268\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 2s 242ms/step - loss: 0.3900 - r2_score: -47.4545 - val_loss: 0.3517 - val_r2_score: -15.8412\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 2s 242ms/step - loss: 0.3719 - r2_score: -94.1628 - val_loss: 0.3346 - val_r2_score: -23.5139\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 2s 242ms/step - loss: 0.3383 - r2_score: -29.2545 - val_loss: 0.3026 - val_r2_score: -8.6237\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 2s 241ms/step - loss: 0.3261 - r2_score: -90.6279 - val_loss: 0.2934 - val_r2_score: -22.4712\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 2s 240ms/step - loss: 0.3060 - r2_score: -74.4050 - val_loss: 0.2685 - val_r2_score: -13.0859\n",
      "2/2 [==============================] - 1s 60ms/step\n",
      "значение -2.543741464733455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return 1 - SS_res/(SS_tot + K.epsilon())\n",
    "\n",
    "def create_model(filters, kernel_size, units, dropout_rate, optimizer_model, gru_act, conv_act):\n",
    "    input_shape = (dataset.x_train.shape[1], 1)\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=conv_act, input_shape=input_shape))\n",
    "    model.add(GRU(units, input_shape=input_shape, activation=gru_act))\n",
    "    model.add(Dense(1024, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.01)))  # L2 регуляризация с коэффициентом 0.01\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.01)))  # L2 регуляризация с коэффициентом 0.01\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))  # L2 регуляризация с коэффициентом 0.01\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))  # L2 регуляризация с коэффициентом 0.01\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))  # L2 регуляризация с коэффициентом 0.01\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer_model, loss='mean_squared_error', metrics=[r2_score])\n",
    "    return model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def create_advanced_model():\n",
    "    model = Sequential()\n",
    "    input_shape = (dataset.x_train.shape[1], 1)\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=[r2_score])\n",
    "    return model\n",
    "\n",
    "dataset = splited_datasets[-1]\n",
    "\"\"\"\n",
    "regressor = KerasRegressor(build_fn=create_model, filters=32, kernel_size=3, units=64, dropout_rate=0.2, optimizer_model='adam', gru_act='tanh', conv_act='relu', epochs=20, batch_size=32)\n",
    "\"\"\"\n",
    "regressor = KerasRegressor(build_fn=create_advanced_model(), epochs=20, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "regressor.fit(dataset.x_train, dataset.y_train, validation_data = (dataset.x_test, dataset.y_test))\n",
    "print(\"значение\", regressor.score(dataset.x_test, dataset.y_test))\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "51/51 [==============================] - 13s 225ms/step - loss: 0.0280 - val_loss: 0.0192\n",
      "Epoch 2/3\n",
      "51/51 [==============================] - 11s 220ms/step - loss: 0.0216 - val_loss: 0.0168\n",
      "Epoch 3/3\n",
      "51/51 [==============================] - 11s 221ms/step - loss: 0.0203 - val_loss: 0.0168\n",
      "dataset: only_percent_dataset\n",
      "Best hyper parameters:  {'conv_act': 'relu', 'dropout_rate': 0.1, 'filters': 8, 'gru_act': 'relu', 'kernel_size': 3, 'optimizer': 'rmsprop', 'units': 64}\n",
      "13/13 [==============================] - 1s 69ms/step\n",
      "MSE: 0.32211055795821064\n",
      "\n",
      "Epoch 1/3\n",
      "51/51 [==============================] - 27s 500ms/step - loss: 0.0284 - val_loss: 0.0229\n",
      "Epoch 2/3\n",
      "51/51 [==============================] - 25s 487ms/step - loss: 0.0256 - val_loss: 0.0222\n",
      "Epoch 3/3\n",
      "51/51 [==============================] - 25s 490ms/step - loss: 0.0246 - val_loss: 0.0211\n",
      "dataset: original_data\n",
      "Best hyper parameters:   {'conv_act': 'tanh', 'dropout_rate': 0.2, 'filters': 8, 'gru_act': 'relu', 'kernel_size': 3, 'optimizer': 'adam', 'units': 64}\n",
      "13/13 [==============================] - 2s 125ms/step\n",
      "MSE: 0.3549903756538161\n"
     ]
    }
   ],
   "source": [
    "def create_model(filters, kernel_size, units, dropout_rate, optimizer_model, gru_act, conv_act):\n",
    "    input_shape = (dataset.get('X_train').shape[1], 1)\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=conv_act, input_shape=input_shape))\n",
    "    model.add(GRU(units, input_shape=input_shape, activation=gru_act))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))  # L2 регуляризация с коэффициентом 0.01\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))  # L2 регуляризация с коэффициентом 0.01\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer_model, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    regressor = KerasRegressor(build_fn=create_model, filters=32, kernel_size=3, units=64, dropout_rate=0.2, optimizer_model='adam', gru_act='tanh', conv_act='relu', epochs=3, batch_size=32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    grid = GridSearchCV(estimator=regressor, param_grid=param_grid_1d_gru, n_jobs=7)\n",
    "    grid_result = grid.fit(dataset.get('X_train'), dataset.get('Y_train'), validation_data = (dataset.get('X_test'), dataset.get('Y_test')))\n",
    "    print(f'dataset: {dataset.get(\"name\")}')\n",
    "    print(\"Best hyper parameters: \", grid_result.best_params_)\n",
    "    print(\"MSE:\", grid.score(dataset.get('X_test'), dataset.get('Y_test')))\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "param_grid_1d_lstm = {\n",
    "    'filters': [8, 32],\n",
    "    'kernel_size': [1, 3],\n",
    "    'units': [16, 64],\n",
    "    'optimizer' : [\"adam\", \"rmsprop\"],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'lstm_act': ['tanh', 'relu'],\n",
    "    'conv_act': ['tanh', 'relu']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "51/51 [==============================] - 15s 261ms/step - loss: 0.0252 - val_loss: 0.0179\n",
      "Epoch 2/3\n",
      "51/51 [==============================] - 13s 262ms/step - loss: 0.0215 - val_loss: 0.0164\n",
      "Epoch 3/3\n",
      "51/51 [==============================] - 13s 259ms/step - loss: 0.0212 - val_loss: 0.0157\n",
      "dataset: only_percent_dataset\n",
      "Best hyper parameters: {'conv_act': 'tanh', 'dropout_rate': 0.3, 'filters': 32, 'kernel_size': 3, 'lstm_act': 'relu', 'optimizer': 'rmsprop', 'units': 64}\n",
      "13/13 [==============================] - 1s 79ms/step\n",
      "MSE: 0.3658389029515866\n",
      "\n",
      "Epoch 1/3\n",
      "51/51 [==============================] - 30s 540ms/step - loss: 0.0329 - val_loss: 0.0263\n",
      "Epoch 2/3\n",
      "51/51 [==============================] - 27s 540ms/step - loss: 0.0262 - val_loss: 0.0222\n",
      "Epoch 3/3\n",
      "51/51 [==============================] - 28s 542ms/step - loss: 0.0248 - val_loss: 0.0221\n",
      "dataset: original_data\n",
      "Best hyper parameters: {'conv_act': 'tanh', 'dropout_rate': 0.1, 'filters': 32, 'kernel_size': 1, 'lstm_act': 'relu', 'optimizer': 'adam', 'units': 64}\n",
      "13/13 [==============================] - 2s 158ms/step\n",
      "MSE: 0.32444082352535886\n"
     ]
    }
   ],
   "source": [
    "def create_model(filters, kernel_size, units, dropout_rate, optimizer_model, lstm_act, conv_act):\n",
    "    input_shape = (dataset.get('X_train').shape[1], 1)\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=conv_act, input_shape=input_shape))\n",
    "    model.add(LSTM(units, activation=lstm_act, input_shape=input_shape))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer_model, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    regressor = KerasRegressor(build_fn=create_model, filters=32, kernel_size=3, units=64, dropout_rate=0.2, optimizer_model='adam', lstm_act='tanh', conv_act='relu', epochs=3, batch_size=32)\n",
    "    grid = GridSearchCV(estimator=regressor, param_grid=param_grid_1d_lstm, n_jobs=7)\n",
    "    grid_result = grid.fit(dataset.get('X_train'), dataset.get('Y_train'), validation_data = (dataset.get('X_test'), dataset.get('Y_test')))\n",
    "    print(f'dataset: {dataset.get(\"name\")}')\n",
    "    print(\"Best hyper parameters: \", grid_result.best_params_)\n",
    "    print(\"MSE:\", grid.score(dataset.get('X_test'), dataset.get('Y_test')))\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "param_grid_gru_lstm = {\n",
    "    'filters': [8, 32],\n",
    "    'kernel_size': [1, 3],\n",
    "    'units': [16, 64],\n",
    "    'units_gru': [16, 64],\n",
    "    'optimizer' : [\"adam\", \"rmsprop\"],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'lstm_act': ['relu'],\n",
    "    'conv_act': ['relu'],\n",
    "    'gru_act': ['relu']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "51/51 [==============================] - 23s 411ms/step - loss: 0.0307 - val_loss: 0.0234\n",
      "Epoch 2/3\n",
      "51/51 [==============================] - 20s 403ms/step - loss: 0.0252 - val_loss: 0.0188\n",
      "Epoch 3/3\n",
      "51/51 [==============================] - 21s 403ms/step - loss: 0.0213 - val_loss: 0.0168\n",
      "dataset: only_percent_dataset\n",
      "Best hyper parameters: {'conv_act': 'relu', 'dropout_rate': 0.1, 'filters': 32, 'gru_act': 'relu', 'kernel_size': 3, 'lstm_act': 'relu', 'optimizer': 'rmsprop', 'units': 64, 'units_gru': 16}\n",
      "13/13 [==============================] - 2s 113ms/step\n",
      "MSE: 0.32369279000082507\n",
      "\n",
      "Epoch 1/3\n",
      "51/51 [==============================] - 56s 1s/step - loss: 0.0363 - val_loss: 0.0300\n",
      "Epoch 2/3\n",
      "51/51 [==============================] - 54s 1s/step - loss: 0.0287 - val_loss: 0.0217\n",
      "Epoch 3/3\n",
      "51/51 [==============================] - 54s 1s/step - loss: 0.0242 - val_loss: 0.0203\n",
      "dataset: original_data\n",
      "Best hyper parameters: {'conv_act': 'relu', 'dropout_rate': 0.2, 'filters': 32, 'gru_act': 'relu', 'kernel_size': 3, 'lstm_act': 'relu', 'optimizer': 'adam', 'units': 64, 'units_gru': 64}\n",
      "13/13 [==============================] - 4s 256ms/step\n",
      "MSE: 0.37896665971742893\n"
     ]
    }
   ],
   "source": [
    "def create_model(filters, kernel_size, units, units_gru, dropout_rate, optimizer_model, lstm_act, gru_act, conv_act):\n",
    "    input_shape = (dataset.get('X_train').shape[1], 1)\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=conv_act, input_shape=input_shape))\n",
    "    model.add(LSTM(units, activation=lstm_act, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(GRU(units_gru, input_shape=input_shape, activation=gru_act))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer_model, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "for dataset in datasets:\n",
    "    regressor = KerasRegressor(build_fn=create_model, filters=32, kernel_size=3, units=64, units_gru=64, dropout_rate=0.2, optimizer_model='adam', lstm_act='tanh', conv_act='relu', gru_act='relu', epochs=3, batch_size=32)\n",
    "    grid = GridSearchCV(estimator=regressor, param_grid=param_grid_gru_lstm, n_jobs=7)\n",
    "    grid_result = grid.fit(dataset.get('X_train'), dataset.get('Y_train'), validation_data = (dataset.get('X_test'), dataset.get('Y_test')))\n",
    "    print(f'dataset: {dataset.get(\"name\")}')\n",
    "    print(\"Best hyper parameters: \", grid_result.best_params_)\n",
    "    print(\"MSE:\", grid.score(dataset.get('X_test'), dataset.get('Y_test')))\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
